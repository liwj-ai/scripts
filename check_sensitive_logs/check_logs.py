import os
import requests
import zipfile
import io
import json
import shutil
from datetime import datetime, timedelta

# ä»ç¯å¢ƒå˜é‡è¯»å– GitHub ç›¸å…³ä¿¡æ¯
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN")
OWNER = os.getenv("OWNER")
REPO = os.getenv("REPO")
WX_WEBHOOK_KEY = os.getenv("WX_WEBHOOK_KEY")
# å…³é”®å­—åˆ—è¡¨ï¼ˆä»ç¯å¢ƒå˜é‡è·å–ï¼Œå¤šä¸ªå…³é”®å­—ç”¨ "," åˆ†éš”ï¼‰
SENSITIVE_KEYWORDS = os.getenv("SENSITIVE_KEYWORDS", "").split(",")

# è®¡ç®—æ˜¨å¤©çš„æ—¶é—´èŒƒå›´ï¼ˆUTC æ—¶é—´ï¼‰
yesterday = datetime.utcnow() - timedelta(days=1)
START_TIME = yesterday.strftime("%Y-%m-%dT00:00:00Z")
# END_TIME = yesterday.strftime("%Y-%m-%dT23:59:59Z")
END_TIME = datetime.now().strftime("%Y-%m-%dT%H:%M:%SZ")
# START_TIME = "2025-02-06T00:00:00Z"
# END_TIME = "2025-02-06T23:59:59Z"


HEADERS = {
    "Authorization": f"Bearer {GITHUB_TOKEN}",
    "Accept": "application/vnd.github.v3+json",
}

def get_failed_runs():
    url = f"https://api.github.com/repos/{OWNER}/{REPO}/actions/runs?status=failure&per_page=100"
    failed_runs = []

    while url:
        response = requests.get(url, headers=HEADERS)
        response.raise_for_status()
        data = response.json()

        for run in data.get("workflow_runs", []):
            created_at = run["created_at"]
            if START_TIME <= created_at <= END_TIME:
                failed_runs.append(run["id"])

        # è·å–åˆ†é¡µé“¾æ¥ï¼ˆGitHub API çš„ Link å¤´éƒ¨ä¼šæä¾›ä¸‹ä¸€é¡µé“¾æ¥ï¼‰
        url = response.links.get("next", {}).get("url")

    print(f"è·å–åˆ° {len(failed_runs)} ä¸ªå¤±è´¥çš„ workflow runs")
    return failed_runs

def download_logs(run_id):
    """ ä¸‹è½½æŒ‡å®š run_id çš„æ—¥å¿—ï¼Œå¹¶ä¿å­˜åˆ°æœ¬åœ° logs.zip """
    url = f"https://api.github.com/repos/{OWNER}/{REPO}/actions/runs/{run_id}/logs"

    response = requests.get(url, headers=HEADERS, stream=True)
    if response.status_code == 404:
        print(f"âŒ æ—¥å¿— {run_id} ä¸å­˜åœ¨ï¼Œå¯èƒ½è¢«åˆ é™¤")
        return None
    response.raise_for_status()

    # å°† ZIP æ–‡ä»¶åˆ†å—å†™å…¥ï¼Œç¡®ä¿å®Œæ•´ä¸‹è½½
    zip_path = f"logs_{run_id}.zip"
    with open(zip_path, "wb") as f:
        shutil.copyfileobj(response.raw, f)  # æŒ‰å—å†™å…¥ï¼Œé¿å… content é•¿åº¦ä¸å®Œæ•´

    print(f"âœ… æ—¥å¿—å·²å®Œæ•´ä¸‹è½½åˆ° {zip_path}")

    # è§£å‹åˆ° logs_{run_id}/ ç›®å½•
    extract_path = f"logs_{run_id}/"
    os.makedirs(extract_path, exist_ok=True)

    with zipfile.ZipFile(zip_path, "r") as zip_ref:
        zip_ref.extractall(extract_path)

    print(f"âœ… æ—¥å¿—å·²è§£å‹åˆ° {extract_path}")

    return extract_path  # è¿”å›è§£å‹åçš„ç›®å½•è·¯å¾„

def search_sensitive_info(logs_dir):
    """ åœ¨è§£å‹åçš„æ—¥å¿—æ–‡ä»¶ä¸­æŸ¥æ‰¾æ•æ„Ÿå…³é”®å­— """
    matches = []

    for root, _, files in os.walk(logs_dir):
        for file in files:
            log_path = os.path.join(root, file)
            with open(log_path, "r", encoding="utf-8", errors="ignore") as f:
                for line in f:
                    for keyword in SENSITIVE_KEYWORDS:
                        if keyword in line:
                            return True

    return False

def check_empty_dirs(logs_dir):
    may_matches = []
    """ æ£€æŸ¥ logs_dir ä¸‹çš„å­æ–‡ä»¶å¤¹æ˜¯å¦ä¸ºç©ºï¼Œè‹¥ä¸ºç©ºåˆ™è®°å½• """
    for sub_dir in os.listdir(logs_dir):
        sub_dir_path = os.path.join(logs_dir, sub_dir)
        if os.path.isdir(sub_dir_path):
            # æ£€æŸ¥å­æ–‡ä»¶å¤¹æ˜¯å¦ä¸ºç©º
            if not any(os.scandir(sub_dir_path)):  # å­æ–‡ä»¶å¤¹ä¸ºç©º
                return True

    return False

def send_markdown_webhook(msg, url):
    webhook_url = f"https://qyapi.weixin.qq.com/cgi-bin/webhook/send?key={WX_WEBHOOK_KEY}"

    # æ ¼å¼åŒ–Markdownæ¶ˆæ¯
    content = f"**{msg}:** {url}"

    payload = {
        "msgtype": "markdown",
        "markdown": {
            "content": content
        }
    }

    response = requests.post(webhook_url, json=payload)
    return response.status_code, response.text

if __name__ == "__main__":
    print(f"ğŸ” è·å– {START_TIME} ~ {END_TIME} å¤±è´¥çš„ workflow logs...")
    failed_runs = get_failed_runs()
    print(f"âœ… æ‰¾åˆ° {len(failed_runs)} ä¸ªå¤±è´¥è¿è¡Œ")
    matches = ""
    may_matches = ""
    for run_id in failed_runs:
        web_url = f"https://github.com/{OWNER}/{REPO}/actions/runs/{run_id}"
        print(f"â¬‡ï¸ ä¸‹è½½æ—¥å¿— {run_id}...")
        logs_dir = download_logs(run_id)
        if not logs_dir:
            continue
        print(f"ğŸ” æœç´¢æ•æ„Ÿå…³é”®å­—...")
        if search_sensitive_info(logs_dir):
            print(f"ğŸš¨ å‘ç°æ•æ„Ÿä¿¡æ¯æ³„éœ²ï¼run_id: {run_id}")
            matches += f"\n\n[{run_id}]({web_url}) "
        elif check_empty_dirs(logs_dir):
            print(f"ğŸš¨ å‘ç°æ•æ„Ÿä¿¡æ¯å¯èƒ½æ³„éœ²ï¼run_id: {run_id}")
            may_matches += f"\n\n[{run_id}]({web_url}) "
        else:
            print(f"âœ… run_id {run_id} æ—¥å¿—æ— æ•æ„Ÿä¿¡æ¯")
    if matches:
        msg = "æ•æ„Ÿä¿¡æ¯æ³„éœ²"
        send_markdown_webhook(msg, matches)
    if may_matches:
        msg = "æ•æ„Ÿä¿¡æ¯å¯èƒ½æ³„éœ²"
        send_markdown_webhook(msg, may_matches)
